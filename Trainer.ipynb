{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e9d51a-91ca-47af-b2ed-75ce95d13778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import utils.environment\n",
    "    env_opts = utils.environment.get_env()\n",
    "except ImportError:\n",
    "    import os, subprocess, sys\n",
    "    assert os.getcwd() == \"/content\"\n",
    "    subprocess.run(\"git clone https://github.com/yamatteo/liver.git /content/liver\".split())\n",
    "    os.chdir(\"/content/liver\")\n",
    "    import utils.environment\n",
    "    utils.environment.reset_environment()\n",
    "    env_opts = utils.environment.get_env()\n",
    "\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import pickle\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import nibabel\n",
    "import numpy as np\n",
    "import torch\n",
    "from adabelief_pytorch import AdaBelief\n",
    "from rich.progress import Progress\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "from torch.utils.data import DataLoader\n",
    "from rich import print\n",
    "\n",
    "import newmodel\n",
    "import report\n",
    "import utils.ndarray as nd\n",
    "import utils.path_explorer as px\n",
    "from distances import liverscore, tumorscore\n",
    "from newmodel.data import store_441_dataset, Dataset, BufferDataset\n",
    "from newmodel.models import UNet\n",
    "from newmodel.funet import FUNet\n",
    "from utils.debug import dbg\n",
    "from utils.namespace import Namespace\n",
    "from utils.slices import slices\n",
    "from utils.storage import gen_split_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def trainsetup(opts: Namespace):\n",
    "    self = Namespace()\n",
    "    if opts.arch == \"unet\":\n",
    "        self.model = UNet(**opts)\n",
    "    elif opts.arch == \"funet\":\n",
    "        self.model = FUNet(**opts)\n",
    "    self.model.to(device=opts.device)\n",
    "\n",
    "    try:\n",
    "        if isinstance(opts.resume, str):\n",
    "            model_name = opts.resume\n",
    "        elif isinstance(opts.resume, int) and opts.resume > 0:\n",
    "            model_name = f\"checkpoint{opts.resume:03}.pth\"\n",
    "        else:\n",
    "            model_name = None\n",
    "            print(\"Starting with a new model.\")\n",
    "        if model_name:\n",
    "            self.model.load_state_dict(torch.load(opts.models_path / model_name, map_location=opts.device))\n",
    "            print(f\"Model loaded from {opts.models_path / model_name}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model {opts.models_path / model_name} does not exist. Starting with a new model.\")\n",
    "    \n",
    "    if opts.momentum:\n",
    "        self.model.set_momentum(opts.momentum)\n",
    "\n",
    "    if not hasattr(opts, \"buffer_size\"):\n",
    "        opts.buffer_size = 0\n",
    "    if opts.buffer_size > 0:\n",
    "        self.train_dataset = BufferDataset(opts.dataset_path / \"train\", opts.buffer_size, opts.buffer_size // 10)\n",
    "        dbg(len(list((opts.dataset_path / \"train\").iterdir())))\n",
    "        dbg(len(self.train_dataset.buffer))\n",
    "    else:\n",
    "        self.train_dataset = Dataset(opts.dataset_path / \"train\")\n",
    "        dbg(len(self.train_dataset.files))\n",
    "    \n",
    "    self.valid_dataset = Dataset(opts.dataset_path / \"valid\")\n",
    "    dbg(len(self.valid_dataset.files))\n",
    "\n",
    "    self.tdl = DataLoader(\n",
    "        self.train_dataset,\n",
    "        pin_memory=True,\n",
    "        batch_size=opts.batch_size,\n",
    "    )\n",
    "    self.vdl = DataLoader(\n",
    "        self.valid_dataset,\n",
    "        pin_memory=True,\n",
    "        batch_size=opts.batch_size,\n",
    "    )\n",
    "\n",
    "    self.optimizer = AdaBelief(\n",
    "        self.model.parameters(),\n",
    "        lr=opts.lr,\n",
    "        eps=1e-8,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decouple=False,\n",
    "        rectify=False,\n",
    "        print_change_log=False,\n",
    "    )\n",
    "\n",
    "    self.loss_function = nn.CrossEntropyLoss(torch.tensor([1, 2, 5])).to(device=opts.device)\n",
    "    if opts.buffer_size > 0:\n",
    "        m = nn.CrossEntropyLoss(torch.tensor([1, 2, 5]), reduction=\"none\").to(device=opts.device)\n",
    "        def score_function(pred, segm, keys):\n",
    "            loss = m(pred, segm)\n",
    "            loss = torch.mean(loss, dim=[1, 2, 3])\n",
    "            scores = {k.item(): loss[i].item() for i, k in enumerate(keys)}\n",
    "            return scores\n",
    "        self.score_function = score_function\n",
    "    return Namespace(opts, self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb810b-5db6-4f31-a4e9-958922594576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Submodel loaded from <span style=\"color: #800080; text-decoration-color: #800080\">/home/yamatteo/saved_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">BBE9_400.pth</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Submodel loaded from \u001b[35m/home/yamatteo/saved_models/\u001b[0m\u001b[95mBBE9_400.pth\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Storing dataset:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Storing dataset:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  source = <span style=\"color: #800080; text-decoration-color: #800080\">/mnt/chromeos/MyFiles/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Downloads</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  source = \u001b[35m/mnt/chromeos/MyFiles/\u001b[0m\u001b[95mDownloads\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  target = <span style=\"color: #800080; text-decoration-color: #800080\">/home/yamatteo/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">dataset</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  target = \u001b[35m/home/yamatteo/\u001b[0m\u001b[95mdataset\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>working on: Hum330\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  \u001b[33m...\u001b[0mworking on: Hum330\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">aid.shape =\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "aid.shape =\n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m48\u001b[0m, \u001b[1;36m128\u001b[0m, \u001b[1;36m128\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Setup 0A91\n",
    "env_opts = Namespace(\n",
    "    utils.environment.get_env(),\n",
    "    device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    ")\n",
    "\n",
    "run_opts = Namespace(\n",
    "    batch_size=5,\n",
    "    buffer_size=100,\n",
    "    lr=1e-3 / 20,\n",
    "    momentum = 0.9,\n",
    "    downsampled_resume=\"BBE9_400.pth\",\n",
    "    setup_id = \"0A91\",\n",
    "    store_slice_shape = (512, 512, 16),\n",
    "    store_pool_kernel = (1, 1, 1),\n",
    ")\n",
    "\n",
    "net_opts = Namespace(\n",
    "    downsampled_channels=[4, 48, 64, 80, 96],\n",
    "    funnel_channels=[4+48, 128, 256, 512],\n",
    "    funnel_size = 5,\n",
    "    down_normalization=\"instance\",\n",
    "    up_dropout=\"drop\",\n",
    ")\n",
    "\n",
    "opts = Namespace(env_opts, run_opts, net_opts)\n",
    "\n",
    "model = FUNet(**opts)\n",
    "model.to(device=opts.device)\n",
    "\n",
    "if opts.downsampled_resume:\n",
    "    downsampled_state = torch.load(opts.models_path / opts.downsampled_resume, map_location=opts.device)\n",
    "    downsampled_state = {\n",
    "        key[8:]: parameter\n",
    "        for key, parameter in downsampled_state.items()\n",
    "        if key[:8] == \"model.0.\"\n",
    "    }\n",
    "    model.downsampled_model.load_state_dict(downsampled_state)\n",
    "    dbg(\"Submodel loaded from\", opts.models_path / opts.downsampled_resume)\n",
    "for param in model.downsampled_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def rebuild_dataset():\n",
    "    try:\n",
    "        shutil.rmtree(opts.dataset_path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    finally:\n",
    "        opts.dataset_path.mkdir()\n",
    "    i, k = 0, 0\n",
    "    pooler = nn.AvgPool3d((4, 4, 1))\n",
    "    unpooler = nn.Upsample(scale_factor=(4, 4, 1), mode='trilinear')\n",
    "    print(\"Storing dataset:\")\n",
    "    print(\"  source =\", opts.sources_path)\n",
    "    print(\"  target =\", opts.dataset_path)\n",
    "    (opts.dataset_path / \"train\").mkdir(exist_ok=True)\n",
    "    (opts.dataset_path / \"valid\").mkdir(exist_ok=True)\n",
    "    for case in utils.path_explorer.iter_trainable(opts.sources_path):\n",
    "        print(\"  ...working on:\", case)\n",
    "        case_path = opts.sources_path / case\n",
    "        scan = nd.load_scan_from_regs(case_path)\n",
    "        scan = np.clip(scan, -1024, 1024)\n",
    "        prediction_aid = []\n",
    "        for slice in slices(scan, (512, 512, 16), overlap=False, pad_seed=\"scan\"):\n",
    "            slice = pooler(torch.tensor(slice, dtype=torch.float32, device=opts.device).unsqueeze(0))\n",
    "            aid = model.downsampled_model(slice)\n",
    "            dbg(aid.shape)\n",
    "            aid = unpooler(aid).squeeze().cpu()\n",
    "            prediction_aid.append(aid)\n",
    "            del aid\n",
    "            torch.cuda.empty_cache()\n",
    "        prediction_aid = torch.cat(prediction_aid, dim=3)\n",
    "        dbg(prediction_aid.shape)\n",
    "        assert False\n",
    "        if 5:\n",
    "            if k % 10 == 0:\n",
    "                file_path = (target_path / \"valid\" / f\"{i:06}.pt\")\n",
    "            else:\n",
    "                file_path = (target_path / \"train\" / f\"{i:06}.pt\")\n",
    "            torch.save(gen(scan, segm), file_path)\n",
    "            i += 1\n",
    "        k += 1\n",
    "    print(f\"Train dataset: {len(list((opts.dataset_path / 'train').iterdir()))} items.\")\n",
    "    print(f\"Valid dataset: {len(list((opts.dataset_path / 'valid').iterdir()))} items.\")\n",
    "def generate_dataset(scan, segm):\n",
    "    dgscan = nn.AvgPool3d((4, 4, 1))(scan)\n",
    "    dgpred = model.downsampled_model(dgscan.unsqueeze(0)).squeeze(0)\n",
    "    return {\"scan\": scan, \"dgpred\": dgpred, \"segm\": segm}\n",
    "\n",
    "rebuild_dataset()\n",
    "if not hasattr(opts, \"buffer_size\"):\n",
    "    opts.buffer_size = 0\n",
    "if opts.buffer_size > 0:\n",
    "    train_dataset = BufferDataset(opts.dataset_path / \"train\", opts.buffer_size, opts.buffer_size // 10)\n",
    "    dbg(len(list((opts.dataset_path / \"train\").iterdir())))\n",
    "    dbg(len(train_dataset.buffer))\n",
    "else:\n",
    "    train_dataset = Dataset(opts.dataset_path / \"train\")\n",
    "    dbg(len(train_dataset.files))\n",
    "\n",
    "valid_dataset = Dataset(opts.dataset_path / \"valid\")\n",
    "dbg(len(valid_dataset.files))\n",
    "\n",
    "tdl = DataLoader(\n",
    "    train_dataset,\n",
    "    pin_memory=True,\n",
    "    batch_size=opts.batch_size,\n",
    ")\n",
    "vdl = DataLoader(\n",
    "    valid_dataset,\n",
    "    pin_memory=True,\n",
    "    batch_size=opts.batch_size,\n",
    ")\n",
    "\n",
    "optimizer = AdaBelief(\n",
    "    model.funnel_model.parameters(),\n",
    "    lr=opts.lr,\n",
    "    eps=1e-8,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decouple=False,\n",
    "    rectify=False,\n",
    "    print_change_log=False,\n",
    ")\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(torch.tensor([1, 2, 5]), reduction=\"none\").to(device=opts.device)\n",
    "def score_function(pred, segm, keys):\n",
    "    loss = loss_function(pred, segm)\n",
    "    loss = torch.mean(loss, dim=[1, 2, 3])\n",
    "    scores = {k.item(): loss[i].item() for i, k in enumerate(keys)}\n",
    "    return scores\n",
    "score_function = score_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2e8e9-eb2f-4734-88e2-cf62b27f6426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Setup BBE9\n",
    "# env_opts = Namespace(\n",
    "#     dataset_path = Path(\"/content/dataset\"),\n",
    "#     device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    "#     drive_folder = Path(drive_mount) / \"MyDrive\" / \"COLAB\",\n",
    "#     models_path = Path(drive_mount) / \"MyDrive\" / \"saved_models\",\n",
    "# )\n",
    "\n",
    "# run_opts = Namespace(\n",
    "#     batch_size=5,\n",
    "#     buffer_size=100,\n",
    "#     lr=1e-3 / 20,\n",
    "#     momentum = 0.9,\n",
    "#     resume=False,\n",
    "#     setup_id = \"BBE9\",\n",
    "#     store_slice_shape = (512, 512, 16),\n",
    "#     store_pool_kernel = (4, 4, 1),\n",
    "# )\n",
    "\n",
    "# net_opts = Namespace(\n",
    "#     channels=[4, 48, 64, 80, 96],\n",
    "#     down_normalization=\"instance\",\n",
    "#     up_dropout=\"drop\",\n",
    "# )\n",
    "\n",
    "# opts = Namespace(env_opts, run_opts, net_opts)\n",
    "\n",
    "# # rebuild_dataset(opts)\n",
    "# setup = trainsetup(opts)\n",
    "# newmodel.train(setup, epochs=401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295fcda-5fc2-4954-a46e-a3ba7be6cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Setup 8C87\n",
    "# env_opts = Namespace(\n",
    "#     dataset_path = Path(\"/content/dataset\"),\n",
    "#     device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    "#     drive_folder = Path(drive_mount) / \"MyDrive\" / \"COLAB\",\n",
    "#     models_path = Path(drive_mount) / \"MyDrive\" / \"saved_models\",\n",
    "# )\n",
    "\n",
    "# run_opts = Namespace(\n",
    "#     batch_size=5,\n",
    "#     buffer_size=100,\n",
    "#     lr=1e-3 / 20,\n",
    "#     momentum = 0.9,\n",
    "#     resume=False,\n",
    "#     setup_id = \"8C87\",\n",
    "# )\n",
    "\n",
    "# net_opts = Namespace(\n",
    "#     channels=[4, 32, 48, 64],\n",
    "#     down_normalization=\"instance\",\n",
    "# )\n",
    "\n",
    "# opts = Namespace(env_opts, run_opts, net_opts)\n",
    "\n",
    "# rebuild_dataset((512, 512, 16), opts)\n",
    "# setup = trainsetup(opts)\n",
    "# newmodel.train(setup, epochs=201)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
